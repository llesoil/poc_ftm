{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code - To put in a python file at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Node, Tree\n",
    "from tree_sitter import Language, Parser\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import graphviz\n",
    "from IPython.display import IFrame\n",
    "\n",
    "if not os.path.isdir('./tree-sitter-go'):\n",
    "    print(\"Cloning https://github.com/tree-sitter/tree-sitter-go\")\n",
    "    os.system(\"git clone https://github.com/tree-sitter/tree-sitter-go\")\n",
    "            \n",
    "Language.build_library('build/my-languages.so', ['tree-sitter-go'])\n",
    "GO_LANGUAGE = Language('build/my-languages.so', 'go')\n",
    "parser = Parser()\n",
    "parser.set_language(GO_LANGUAGE)\n",
    "\n",
    "class ToggleCatcher():\n",
    "    \n",
    "    def __init__(self, system_name):\n",
    "        \n",
    "        self.system_name = system_name\n",
    "        \n",
    "        # configuration file containing the json - see the HowTo.md \n",
    "        # to add your own configuration\n",
    "        self.config_file_path = \"./config/\"+self.system_name+'.json'\n",
    "        \n",
    "        # directory to export the results\n",
    "        self.results_dir = \"./results/\"\n",
    "        \n",
    "        # inputs - properties of the current system\n",
    "        attributes = json.load(open(self.config_file_path))\n",
    "        ## directory in which the source code is located\n",
    "        ## e.g. \"./kops/\" for Kops\n",
    "        self.directory = attributes[\"directory\"]\n",
    "        ## if the directory of the system does not exist, we clone the repository\n",
    "        if not os.path.isdir(self.directory):\n",
    "            print(\"Cloning \"+attributes[\"url\"])\n",
    "            os.system(\"git clone \"+attributes[\"url\"])\n",
    "\n",
    "        ## if we have access to the keywords, we use them\n",
    "        if \"keywords\" in attributes:\n",
    "            self.keywords = [k.lower() for k in attributes[\"keywords\"]]\n",
    "        else:\n",
    "            ## else we find them with the regular expression\n",
    "            ## the file listing the Feature Toggles (FT) and their name \n",
    "            ## aka our keywords to search in the code\n",
    "            ## e.g. \"./kops/pkg/featureflag/featureflag.go\" for Kops\n",
    "            self.ft_file = attributes[\"ft_file\"]\n",
    "            ## the regular expression to use to find the names of FTs\n",
    "            ## e.g. \"[N|n]ew.*,*Bool*\" for Kops\n",
    "            self.reg_exp = attributes[\"reg_exp\"]\n",
    "            self.sep = None\n",
    "            if \"sep_reg_exp\" in attributes:\n",
    "                self.sep = attributes['sep_reg_exp']\n",
    "            self.keywords = self.get_ft_keywords()\n",
    "        ## the way ft are expressed in the code\n",
    "        ## depends on the library used by the developers\n",
    "        ## e.g. \"featureflag.\" for Kops\n",
    "        self.feature_structure = attributes[\"feature_structure\"]\n",
    "        \n",
    "        # outputs - measures on the system\n",
    "        ## counts the number of files with FT\n",
    "        self.count_file = dict()\n",
    "        ## counts the occurrences of FTs in the files\n",
    "        self.occurences = dict()\n",
    "        ## lists keywords per file\n",
    "        self.kw_file = dict()\n",
    "        ## the resulting list of interesting statements\n",
    "        self.statements = []\n",
    "        ## initialize the dicts\n",
    "        for kw in sorted(self.keywords):\n",
    "            self.count_file[kw] = 0\n",
    "            self.occurences[kw] = 0\n",
    "        \n",
    "        # files with fts\n",
    "        self.file_interests = self.list_kw_files()\n",
    "        \n",
    "        # launches an analyse\n",
    "        self.analyse_all_files()\n",
    "\n",
    "    def get_ft_keywords(self):\n",
    "        ## output : search keywords in the file self.ft_file containing all the feature toggles\n",
    "        ## uses the regular_expression self.reg_exp to search in the file\n",
    "        \n",
    "        with open(self.ft_file, 'r') as f:\n",
    "            catch_feat = re.findall(self.reg_exp, f.read())\n",
    "        \n",
    "        if self.sep:\n",
    "            keywords = [feature.split(self.sep)[1] for feature in catch_feat]\n",
    "        else:\n",
    "            keywords = [feature for feature in catch_feat]\n",
    "\n",
    "        return [k.lower() for k in keywords]\n",
    "    \n",
    "    def list_kw_files(self):\n",
    "        ### output : lists the files with feature toggles based on the self.keywords list of FTs\n",
    "        \n",
    "        go_folders = [x[0] for x in os.walk(self.directory)]\n",
    "\n",
    "        go_files = []\n",
    "        for dir_name in go_folders:\n",
    "            files = [dir_name+\"/\"+k for k in os.listdir(dir_name) if k[len(k)-3:] ==\".go\"]\n",
    "            # '.go' could be an input the language in the next version in the configuration file\n",
    "            go_files.extend(files)\n",
    "\n",
    "        go_file_interests = []\n",
    "\n",
    "        for file in go_files:\n",
    "            s = \"\"\n",
    "            with open(file, \"r\") as f:\n",
    "                s+=f.read().lower()\n",
    "            kws = [k for k in self.keywords if k in s]\n",
    "            if len(kws) > 0 and self.feature_structure in s:\n",
    "                self.kw_file[file] = []\n",
    "                for k in kws:\n",
    "                    self.kw_file[file].append(k)\n",
    "                    if self.feature_structure+k in s:\n",
    "                        self.count_file[k]+=1\n",
    "                    self.occurences[k]+=s.count(self.feature_structure+k)\n",
    "                go_file_interests.append(file)\n",
    "\n",
    "        return go_file_interests\n",
    "\n",
    "    \n",
    "    def get_code(self, node):\n",
    "        ### input : a node of the ast\n",
    "        ### output : get the \"code\" of the node of the ast, \n",
    "        ### i.e. the string content of the related part in the code\n",
    "        \n",
    "        code = self.source[node.start_byte:node.end_byte].decode('utf8').lower()\n",
    "        code = code.replace('\\n', '').replace('\\t','')\n",
    "        return code\n",
    "\n",
    "    \n",
    "    def get_id(self, node):\n",
    "        ### input : a node of the ast\n",
    "        ### output : get the \"code\" of the node, i.e. the related part of the code\n",
    "        \n",
    "        node_type = node.type\n",
    "        if node_type not in self.type_nodes:\n",
    "            self.type_nodes[node_type]=1\n",
    "        else:\n",
    "            self.type_nodes[node_type]+=1\n",
    "        return node_type+str(self.type_nodes[node_type])\n",
    "\n",
    "    \n",
    "    def process_node(self, root_id, node):\n",
    "        ### input : a parent node and a child node\n",
    "        ### output : analysis of the parent and starts the analyses of the grandchildren\n",
    "        ### works recursively\n",
    "        ### analyse = isolates the conditions of the if statements containing at least one feature toggle\n",
    "        ### @Aaron to replace with your code when you have the time\n",
    "        \n",
    "        node_id = self.get_id(node)\n",
    "        node_content = self.get_code(node)\n",
    "        for kw in self.keywords:\n",
    "            if kw in node_content and node.type == 'if_statement':\n",
    "                for c in node.children:\n",
    "                    if c.type in ['binary_expression', 'unary_expression', 'call_expression']:\n",
    "                        final_code = self.get_code(c)\n",
    "                        if self.feature_structure in final_code:\n",
    "                            self.statements.append(final_code)\n",
    "        if len(node.children) != 0:\n",
    "            for i in range(len(node.children)):\n",
    "                self.process_node(node_id, node.children[i])\n",
    "    \n",
    "    \n",
    "    def analyse_file(self, filename):\n",
    "        ### input : a filename\n",
    "        ### output: process the ast of one file\n",
    "        \n",
    "        s = \"\"\n",
    "        with open(filename, \"r\") as f:\n",
    "            s+=f.read()+\"\\n\"\n",
    "        s = s.lower()\n",
    "\n",
    "        self.source = bytes(s, \"utf8\")\n",
    "        ast = parser.parse(self.source)\n",
    "\n",
    "        root_node = ast.root_node\n",
    "\n",
    "        self.type_nodes = dict()\n",
    "\n",
    "        for i in range(len(root_node.children)):\n",
    "            self.process_node(\"root\", root_node.children[i])\n",
    "    \n",
    "    def analyse_all_files(self):\n",
    "        ### output : analyse all the .go files of the project\n",
    "        ### and put the results in self.statements\n",
    "        \n",
    "        for fi in self.file_interests:\n",
    "            self.analyse_file(fi)\n",
    "            \n",
    "    def export_results(self):\n",
    "        #### output: export results to the ./results dir\n",
    "        print(\"------\\nExporting\", system, \"results\\n------\")\n",
    "        print(\"Saving statements to \", \n",
    "              self.results_dir+\"statements/\"+system+\".txt\")\n",
    "        np.savetxt(self.results_dir+\"statements/\"+self.system_name+\".txt\", \n",
    "                   self.statements, fmt=\"%s\", delimiter=\"\\n\")\n",
    "        print(\"Saving other results to \", \n",
    "              self.results_dir+'kw_file/'+self.system_name+'.json',\n",
    "              self.results_dir+'occurences/'+self.system_name+'.json',\n",
    "              self.results_dir+'count_file/'+self.system_name+'.json')\n",
    "        with open(self.results_dir+'kw_file/'+self.system_name+'.json', \"w\") as outfile:\n",
    "            json.dump(tc.kw_file, outfile)\n",
    "        with open(self.results_dir+'occurences/'+self.system_name+'.json', \"w\") as outfile:\n",
    "            json.dump(tc.occurences, outfile)\n",
    "        with open(self.results_dir+'count_file/'+self.system_name+'.json', \"w\") as outfile:\n",
    "            json.dump(tc.count_file, outfile)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    def process_statement(self, dico, logic_link, statement):\n",
    "        ### inputs: a dico to increment, a statement to analyse and the type of logical links\n",
    "        ### between the expressions\n",
    "        ### output: the dico incremented\n",
    "        tab = statement.split(logic_link)\n",
    "        res = ['expr']*len(tab)\n",
    "        for i in range(len(tab)):\n",
    "            t = tab[i]\n",
    "            for kw in self.keywords:\n",
    "                if kw in t:\n",
    "                    res[i] = kw\n",
    "        for i in range(len(res)):\n",
    "            first = res[i]\n",
    "            for j in range(i+1, len(res)):\n",
    "                second = res[j]\n",
    "                index = (min(first, second), max(first, second))\n",
    "                if index not in dico:\n",
    "                    dico[index] = 1\n",
    "                else:\n",
    "                    dico[index]+=1\n",
    "        return dico\n",
    "        \n",
    "    def analyse_statements(self):\n",
    "        ### output: three distionaries, containing the relationships between feature toggles\n",
    "        ### and_rel the relations of intersection\n",
    "        ### or_rel the relations of union\n",
    "        ### alone the number of solely call of FT\n",
    "        \n",
    "        and_rel = dict()\n",
    "        or_rel = dict()\n",
    "        alone = dict()\n",
    "\n",
    "        for st in self.statements:\n",
    "            if '&&' in st:\n",
    "                and_rel = self.process_statement(and_rel, '&&', st)\n",
    "            elif '||' in st:\n",
    "                or_rel = self.process_statement(or_rel, '||', st)\n",
    "            else:\n",
    "                res = 'expr'\n",
    "                for kw in self.keywords:\n",
    "                    if kw in st:\n",
    "                        res = kw\n",
    "                if res not in alone:\n",
    "                    alone[res] = 1\n",
    "                else:\n",
    "                    alone[res]+=1\n",
    "        \n",
    "        return (alone, or_rel, and_rel)\n",
    "    \n",
    "    def build_ftm(self):\n",
    "        ### input : three dictionaries, see analyse_statements above\n",
    "        ### output : graphical representation of FTM\n",
    "        \n",
    "        alone, or_rel, and_rel = self.analyse_statements()\n",
    "        \n",
    "        ftm = graphviz.Graph(comment='FTM '+system)\n",
    "\n",
    "        for al in alone:\n",
    "            if al != 'expr':\n",
    "                ftm.node(al, al)\n",
    "                ftm.edge(al, al, label=str(alone[al]))\n",
    "\n",
    "        for orr in or_rel:\n",
    "            ftm.edge(orr[0], orr[1], label=str(or_rel[orr]), color=\"red\")\n",
    "\n",
    "        for andr in and_rel:\n",
    "            ftm.edge(andr[0], andr[1], label=str(and_rel[andr]), color =\"blue\")\n",
    "\n",
    "        ftm.render(self.results_dir+'FTM/'+self.system_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Exporting boulder results\n",
      "------\n",
      "Saving statements to  ./results/statements/boulder.txt\n",
      "Saving other results to  ./results/kw_file/boulder.json ./results/occurences/boulder.json ./results/count_file/boulder.json\n",
      "\n",
      "\n",
      "------\n",
      "Exporting client results\n",
      "------\n",
      "Saving statements to  ./results/statements/client.txt\n",
      "Saving other results to  ./results/kw_file/client.json ./results/occurences/client.json ./results/count_file/client.json\n",
      "\n",
      "\n",
      "------\n",
      "Exporting juju results\n",
      "------\n",
      "Saving statements to  ./results/statements/juju.txt\n",
      "Saving other results to  ./results/kw_file/juju.json ./results/occurences/juju.json ./results/count_file/juju.json\n",
      "\n",
      "\n",
      "------\n",
      "Exporting kops results\n",
      "------\n",
      "Saving statements to  ./results/statements/kops.txt\n",
      "Saving other results to  ./results/kw_file/kops.json ./results/occurences/kops.json ./results/count_file/kops.json\n",
      "\n",
      "\n",
      "------\n",
      "Exporting kubernetes results\n",
      "------\n",
      "Saving statements to  ./results/statements/kubernetes.txt\n",
      "Saving other results to  ./results/kw_file/kubernetes.json ./results/occurences/kubernetes.json ./results/count_file/kubernetes.json\n",
      "\n",
      "\n",
      "------\n",
      "Exporting loomchain results\n",
      "------\n",
      "Saving statements to  ./results/statements/loomchain.txt\n",
      "Saving other results to  ./results/kw_file/loomchain.json ./results/occurences/loomchain.json ./results/count_file/loomchain.json\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_dir = \"./config/\"\n",
    "\n",
    "for config_file in sorted(os.listdir(config_dir)):\n",
    "    system = config_file[:-5]\n",
    "    tc = ToggleCatcher(system)\n",
    "    tc.export_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\begin{tabular}{|c|c|c|c|c|}\n",
      "\\hline\n",
      "System & min \\#Files & max \\#Files & min \\#Toggle Point & max \\#Toggle Point \\\\ \\hline\n",
      "boulder &  1 &  2 &  1 &  4 \\\\ \\hline\n",
      "client &  0 &  19 &  0 &  94 \\\\ \\hline\n",
      "juju &  1 &  12 &  1 &  17 \\\\ \\hline\n",
      "kops &  1 &  11 &  1 &  20 \\\\ \\hline\n",
      "kubernetes &  0 &  23 &  0 &  106 \\\\ \\hline\n",
      "loomchain &  0 &  7 &  0 &  28 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\begin{table}\")\n",
    "print(\"\\\\begin{tabular}{|c|c|c|c|c|}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"System & min \\\\#Files & max \\\\#Files & min \\\\#Toggle Point & max \\\\#Toggle Point \\\\\\\\ \\\\hline\")\n",
    "for config_file in sorted(os.listdir(config_dir)):\n",
    "    system = config_file[:-5]\n",
    "    tc = ToggleCatcher(system)\n",
    "    print(system,\n",
    "          \"& \",  np.min([val for val in tc.count_file.values()]),\n",
    "          \"& \",  np.max([val for val in tc.count_file.values()]),\n",
    "          \"& \",  np.min([val for val in tc.occurences.values()]),\n",
    "          \"& \",  np.max([val for val in tc.occurences.values()]), \n",
    "          \"\\\\\\\\ \\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the statements (conditions of if statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      " Analysing  boulder statements\n",
      "------\n",
      "Alone :  {'allowv1registration': 1, 'restrictrsakeysizes': 1, 'fasternewordersratelimit': 3, 'v1disablenewvalidations': 1, 'serverenewalinfo': 3, 'multivafullresults': 1, 'enforcemultiva': 1, 'caaaccounturi': 1, 'caavalidationmethods': 1}\n",
      "Or relations :  {}\n",
      "And relations :  {('ecdsaforall', 'expr'): 3, ('expr', 'expr'): 7, ('expr', 'storerevokerinfo'): 2, ('expr', 'mandatorypostasget'): 8, ('caaaccounturi', 'expr'): 1, ('enforcemultiva', 'multivafullresults'): 2}\n",
      "------\n",
      " Analysing  client statements\n",
      "------\n",
      "Alone :  {'featureflags': 4, 'expr': 7}\n",
      "Or relations :  {}\n",
      "And relations :  {('expr', 'expr'): 1, ('expr', 'featureflags'): 1}\n",
      "------\n",
      " Analysing  juju statements\n",
      "------\n",
      "Alone :  {'asynchronouscharmdownloads': 1, 'charmassumes': 2, 'rawk8sspec': 1, 'developermode': 4, 'strictmigration': 1, 'legacyupstart': 2, 'raftbatchfsm': 1, 'raftapileases': 1, 'logerrorstack': 1, 'secrets': 2}\n",
      "Or relations :  {('branches', 'generations'): 12}\n",
      "And relations :  {('developermode', 'expr'): 5, ('branches', 'generations'): 2, ('branches', 'expr'): 2, ('expr', 'generations'): 4}\n",
      "------\n",
      " Analysing  kops statements\n",
      "------\n",
      "Alone :  {'spotinst': 8, 'apiservernodes': 5, 'cachenodeidentityinfo': 1, 'alphaallowgce': 2, 'alphaallowali': 2, 'azure': 3, 'spotinsthybrid': 12, 'keeplaunchconfigurations': 1, 'terraformjson': 2, 'terraformmanagedfiles': 3, 'useaddonoperators': 3, 'vpcskipenablednssupport': 1, 'googlecloudbucketacl': 1, 'spotinstocean': 4, 'skipetcdversioncheck': 1, 'experimentalclusterdns': 1, 'vfsvaultsupport': 2, 'specoverrideflag': 3, 'enableseparateconfigbase': 1, 'clusteraddons': 1, 'awsipv6': 1}\n",
      "Or relations :  {('apiservernodes', 'expr'): 1}\n",
      "And relations :  {('expr', 'terraformmanagedfiles'): 1, ('apiservernodes', 'expr'): 2, ('expr', 'useaddonoperators'): 1, ('spotinst', 'spotinstcontroller'): 2, ('expr', 'spotinsthybrid'): 2, ('expr', 'spotinst'): 1, ('alphaallowgce', 'expr'): 1}\n",
      "------\n",
      " Analysing  kubernetes statements\n",
      "------\n",
      "Alone :  {'csimigration': 10, 'expandpersistentvolumes': 7, 'csiinlinevolume': 8, 'csivolumefsgrouppolicy': 2, 'csivolumehealth': 2, 'expandcsivolumes': 1, 'delegatefsgrouptocsidriver': 2, 'sizememorybackedvolumes': 1, 'podoverhead': 2, 'localstoragecapacityisolation': 8, 'podaffinitynamespaceselector': 2, 'defaultpodtopologyspread': 4, 'volumecapacitypriority': 2, 'csimigrationaws': 2, 'csimigrationgce': 2, 'csimigrationazuredisk': 2, 'csimigrationopenstack': 2, 'execprobetimeout': 1, 'apparmor': 1, 'indexedjob': 1, 'suspendjob': 2, 'jobtrackingwithfinalizers': 1, 'csrduration': 3, 'ephemeralcontainers': 11, 'namespacedefaultlabelname': 2, 'servicelbnodeportcontrol': 4, 'mixedprotocollbservice': 2, 'serviceloadbalancerclass': 1, 'serviceinternaltrafficpolicy': 3, 'hpacontainermetrics': 2, 'daemonsetupdatesurge': 3, 'statefulsetminreadyseconds': 8, 'csistoragecapacity': 5, 'networkpolicyendport': 1, 'kubeletinusernamespace': 2, 'expandinusepersistentvolumes': 1, 'nodeswap': 2, 'cpucfsquotaperiod': 2, 'windowshostprocesscontainers': 4, 'disableacceleratorusagemetrics': 2, 'gracefulnodeshutdown': 1, 'kubeletpodresourcesgetallocatable': 2, 'topologymanager': 4, 'qosreserved': 1, 'cpumanager': 6, 'memorymanager': 3, 'memoryqos': 1, 'deviceplugins': 1, 'expandeddnsconfig': 3, 'dynamickubeletconfig': 5, 'poddeletioncost': 1, 'logarithmicscaledown': 2, 'jobreadypods': 2, 'topologyawarehints': 4, 'endpointsliceterminatingcondition': 2, 'hpascaletozero': 1, 'anyvolumedatasource': 2, 'proxyterminatingendpoints': 1, 'winoverlay': 1, 'cpumanagerpolicyoptions': 2, 'kubeletpodresources': 1, 'poddisruptionbudget': 1}\n",
      "Or relations :  {('expr', 'expr'): 1, ('ephemeralcontainers', 'expr'): 2, ('expr', 'localstoragecapacityisolation'): 1}\n",
      "And relations :  {('csimigration', 'expr'): 2, ('expr', 'expr'): 26, ('csiinlinevolume', 'expr'): 9, ('expr', 'readwriteoncepod'): 8, ('readwriteoncepod', 'readwriteoncepod'): 2, ('csivolumehealth', 'expr'): 1, ('expr', 'prefernominatednode'): 1, ('expr', 'volumecapacitypriority'): 2, ('expr', 'podoverhead'): 6, ('expr', 'servicelbnodeportcontrol'): 2, ('expandpersistentvolumes', 'expr'): 3, ('expr', 'indexedjob'): 2, ('expr', 'jobtrackingwithfinalizers'): 1, ('dynamickubeletconfig', 'expr'): 9, ('expr', 'hpacontainermetrics'): 1, ('csistoragecapacity', 'expr'): 4, ('csivolumefsgrouppolicy', 'expr'): 3, ('expr', 'networkpolicyendport'): 1, ('expr', 'windowshostprocesscontainers'): 1, ('ephemeralcontainers', 'expr'): 1, ('disablecloudproviders', 'expr'): 2, ('expr', 'rotatekubeletservercertificate'): 3, ('expandinusepersistentvolumes', 'expr'): 3, ('expr', 'memoryqos'): 5, ('expr', 'kubeletcredentialproviders'): 2, ('expr', 'gracefulnodeshutdown'): 2, ('expr', 'localstoragecapacityisolation'): 3, ('cpucfsquotaperiod', 'cpucfsquotaperiod'): 2, ('topologymanager', 'topologymanager'): 2, ('expr', 'nodeswap'): 1, ('cpumanagerpolicyalphaoptions', 'expr'): 2, ('cpumanagerpolicybetaoptions', 'expr'): 2, ('execprobetimeout', 'expr'): 1, ('indexedjob', 'indexedjob'): 1, ('expr', 'suspendjob'): 2, ('expr', 'statefulsetminreadyseconds'): 11, ('expr', 'serviceinternaltrafficpolicy'): 3, ('expr', 'hpascaletozero'): 1, ('expr', 'nonpreemptingpriority'): 6, ('procmounttype', 'procmounttype'): 1, ('apparmor', 'apparmor'): 1, ('ephemeralcontainers', 'ephemeralcontainers'): 1, ('expr', 'probeterminationgraceperiod'): 1, ('expr', 'identifypodos'): 1, ('expr', 'procmounttype'): 1, ('expr', 'podaffinitynamespaceselector'): 4, ('podaffinitynamespaceselector', 'podaffinitynamespaceselector'): 1, ('expandcsivolumes', 'expr'): 1, ('expr', 'kubeletinusernamespace'): 1, ('expr', 'proxyterminatingendpoints'): 1, ('expr', 'winoverlay'): 1, ('expr', 'windsr'): 1, ('expr', 'ipv6dualstack'): 1, ('delegatefsgrouptocsidriver', 'expr'): 2, ('seccompdefault', 'seccompdefault'): 2}\n",
      "------\n",
      " Analysing  loomchain statements\n",
      "------\n",
      "Alone :  {'evmtxreceiptsversion3_1': 4, 'multichainsigtxmiddlewareversion1_1': 1, 'authsigtxfeatureprefix': 3, 'evmtxreceiptsversion3_4': 8, 'evmtxreceiptsversion3_3': 6, 'evmtxreceiptsversion3_2': 2, 'coinversion1_3feature': 1, 'tgversion1_7': 1, 'coinversion1_1feature': 4, 'coinversion1_2feature': 1, 'dposversion3_6': 1, 'dposversion3_7': 1, 'dposversion3_3': 3, 'dposversion3_4': 5, 'dposversion3_9': 1, 'dposversion3_1': 2, 'dposversion3_2': 2, 'dposversion3_5': 2, 'dposversion3feature': 25, 'dposversion2_1': 1, 'addressmapperversion1_1': 1, 'chaincfgversion1_3': 2, 'chaincfgversion1_1': 1, 'chaincfgversion1_2': 1, 'userdeployerwhitelistversion1_1feature': 2, 'userdeployerwhitelistversion1_2feature': 2, 'migrationtxfeature': 1, 'migrationfeatureprefix': 1, 'migrationtxversion1_1feature': 1, 'ethtxfeature': 2, 'userdeployerwhitelistfeature': 2, 'deployerwhitelistfeature': 1}\n",
      "Or relations :  {('dposversion3feature', 'expr'): 2}\n",
      "And relations :  {('ethtxfeature', 'expr'): 1, ('coinversion1_2feature', 'expr'): 7, ('dposversion3_10', 'expr'): 4, ('dposversion3_9', 'expr'): 1, ('dposversion3_1', 'expr'): 1, ('chaincfgversion1_4', 'expr'): 2}\n"
     ]
    }
   ],
   "source": [
    "for config_file in sorted(os.listdir(config_dir)):\n",
    "    system = config_file[:-5]\n",
    "    tc = ToggleCatcher(system)\n",
    "    print(\"------\\n Analysing \", system, \"statements\\n------\")\n",
    "    print(\"Alone : \", tc.analyse_statements()[0])\n",
    "    print(\"Or relations : \", tc.analyse_statements()[1])\n",
    "    print(\"And relations : \", tc.analyse_statements()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the FTM with Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for config_file in sorted(os.listdir(config_dir)):\n",
    "    system = config_file[:-5]\n",
    "    tc = ToggleCatcher(system)\n",
    "    tc.build_ftm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display an example (or = red, and = blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1500\"\n",
       "            height=\"500\"\n",
       "            src=\"./results/FTM/kops.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7ff8a0a6d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('./results/FTM/kops.pdf', width=1500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
